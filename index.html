<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AbdCTBench: Learning Clinical Biomarker Representations from Abdominal Surface Geometry</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <header>
        <div class="container">
            <div class="stanford-branding">
                <p class="stanford-label">Stanford University</p>
            </div>
            <h1>AbdCTBench</h1>
            <p class="subtitle">Learning Clinical Biomarker Representations from Abdominal Surface Geometry</p>
            <p class="venue">Fourteenth International Conference on Learning Representations (ICLR) 2026</p>
            <div class="header-links">
                <a href="https://openreview.net/forum?id=dKRAo0a9Gm&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2026%2FConference%2FAuthors%23your-submissions)" target="_blank" rel="noopener noreferrer" class="btn-paper">Paper</a>
                <a href="javascript:void(0);" class="btn-github">GitHub</a>
            </div>
        </div>
    </header>

    <nav>
        <div class="container">
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#dataset">Dataset</a></li>
                <li><a href="#benchmark">Benchmark</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#download">Download</a></li>
                <li><a href="#citation">Citation</a></li>
            </ul>
        </div>
    </nav>

    <main>
        <section id="overview" class="section">
            <div class="container">
                <h2>Overview</h2>
                <figure class="overview-panel">
                    <div class="overview-panel-grid">
                        <img src="images/031314-1809.png" alt="Abdominal surface mesh sample 1">
                        <img src="images/081721-2058.png" alt="Abdominal surface mesh sample 2">
                        <img src="images/012219-0822.png" alt="Abdominal surface mesh sample 3">
                        <img src="images/081415-1097.png" alt="Abdominal surface mesh sample 4">
                    </div>
                    <figcaption>
                        Fig 1: Sample 2D abdominal surface meshes from the AbdCTBench dataset. These CT-derived surface geometries demonstrate the range of external anatomical features used to predict internal body composition biomarkers without radiation exposure. Complete biomarker details for these images are provided in the Appendix.
                    </figcaption>
                </figure>
                <div class="content-grid">
                    <div class="text-content">
                        <p>
                            Body composition analysis through CT and MRI imaging provides critical insights for cardiometabolic health assessment but remains limited by accessibility barriers including radiation exposure, high costs, and infrastructure requirements.
                        </p>
                        <p>
                            We present <strong>AbdCTBench</strong>, a large-scale dataset containing <strong>23,506 CT-derived abdominal surface meshes</strong> from <strong>18,719 patients</strong>, paired with <strong>87 comorbidity labels</strong>, <strong>31 specific diagnosis codes</strong>, and <strong>16 CT-derived biomarkers</strong>. Our key insight is that external surface geometry is predictive of internal tissue composition, enabling accessible health screening through consumer devices.
                        </p>
                        <p>
                            We establish comprehensive benchmarks across <strong>seven</strong> computer vision architectures (ResNet-18/34/50, DenseNet-121, EfficientNet-B0, ViT-Small, and Swin Transformer-Base), demonstrating that models can learn robust surface-to-biomarker representations directly from 2D mesh projections. Our best-performing models achieve clinically relevant accuracy: age prediction with MAE 6.22 years (R²=0.757), mortality prediction with AUROC 0.839, and diabetes (with chronic complications) detection with AUROC 0.801.
                        </p>
                    </div>
                    <div class="stats-box">
                        <h3>Key Statistics</h3>
                        <ul>
                            <li><strong>23,506</strong> CT-derived surface meshes</li>
                            <li><strong>18,719</strong> unique patients</li>
                            <li><strong>87</strong> HCC comorbidity labels</li>
                            <li><strong>31</strong> ICD-10 diagnosis codes</li>
                            <li><strong>16</strong> CT-derived biomarkers</li>
                            <li><strong>10</strong> benchmark tasks</li>
                            <li><strong>7</strong> architectures evaluated</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <section id="dataset" class="section alt-bg">
            <div class="container">
                <h2>Dataset</h2>
                <figure class="dataset-figure">
                    <img src="images/AbdCTBench_Figure1.png" alt="Sample 2D abdominal surface meshes from the AbdCTBench dataset">
                    <figcaption>
                        Fig 2: AbdCTBench dataset overview showing the pipeline from CT scans to surface mesh extraction and biomarker prediction.
                    </figcaption>
                </figure>
                
                <h3>Collection and Curation</h3>
                <p>
                    AbdCTBench is a comprehensive dataset derived from <strong>23,506 abdominal CTs of 18,719 patients</strong> (≈1.26 scans per patient), representing one of the largest CT-derived biomarker datasets for abdominal composition analysis. The data was collected from CT scans conducted from August 11, 2003, to September 9, 2021, under IRB approval.
                </p>
                <p>
                    Processing proceeded in two parallel phases:
                </p>
                <ul>
                    <li><strong>Surface mesh rendering:</strong> DICOM image series were converted to STL files, then to 2D PNG images of size 384×384 via PyVista</li>
                    <li><strong>Biomarker calculation:</strong> DICOM series were processed by OSCAR, which creates segmentation masks to calculate metrics at vertebral levels (L1-L5, T10-T12) and organ-specific regions (liver, spleen, kidneys, aorta)</li>
                </ul>

                <h3>Dataset Statistics</h3>
                <div class="stats-grid">
                    <div class="stat-card">
                        <h4>Demographics</h4>
                        <ul>
                            <li>Mean age: 55.3 years (SD: 16.51)</li>
                            <li>56.8% female, 43.2% male</li>
                            <li>Balanced sex distribution</li>
                        </ul>
                    </div>
                    <div class="stat-card">
                        <h4>Clinical Conditions</h4>
                        <ul>
                            <li>Essential hypertension: 53.7%</li>
                            <li>Type 2 Diabetes: 44.6%</li>
                            <li>Impaired glucose tolerance: 38.0%</li>
                            <li>Tobacco use: 26.8%</li>
                            <li>Myocardial Infarction: 23.1%</li>
                        </ul>
                    </div>
                    <div class="stat-card">
                        <h4>HCC Comorbidities</h4>
                        <ul>
                            <li>Average 1.8 HCC conditions per patient</li>
                            <li>HCC 108 (Vascular Disease): 22.6%</li>
                            <li>HCC 19 (Diabetes w/o complications): 13.0%</li>
                            <li>HCC 12 (Cancers): 10.9%</li>
                        </ul>
                    </div>
                    <div class="stat-card">
                        <h4>Biomarkers</h4>
                        <ul>
                            <li>Calcium Scoring Agatston: 1200.9 ± 3126.5</li>
                            <li>Kidney median HU: 90.0 ± 58.8</li>
                            <li>Spleen volume: 223.9 ± 127.2 cm³</li>
                            <li>Comprehensive adipose tissue analysis</li>
                        </ul>
                    </div>
                </div>

                <h3>Data Splits</h3>
                <p>
                    The dataset was split at the patient ID level to prevent data leakage:
                </p>
                <ul>
                    <li><strong>Training:</strong> 70%</li>
                    <li><strong>Validation:</strong> 20%</li>
                    <li><strong>Test:</strong> 10%</li>
                </ul>
                <p>
                    All hyperparameter tuning and model selection used only the train and validation sets. The test set was held out exclusively for final evaluation.
                </p>

                <h3>HIPAA Compliance</h3>
                <p>
                    The dataset was processed for HIPAA Safe Harbor compliance, removing all protected health information (PHI) for safe public release. All patient identifiers were replaced with randomized study IDs, and only anonymized abdominal surface meshes are distributed.
                </p>
            </div>
        </section>

        <section id="benchmark" class="section">
            <div class="container">
                <h2>Benchmark Tasks</h2>
                
                <p>
                    From AbdCTBench, we curate <strong>10 biomarker prediction tasks</strong> from 2D surface mesh images. We design a single-target learning framework to benchmark selected architectures on biomarker prediction.
                </p>

                <h3>Prediction Tasks</h3>
                <div class="tasks-grid">
                    <div class="task-card">
                        <h4>Mortality Prediction</h4>
                        <p>Binary classification for patient death during follow-up</p>
                        <span class="prevalence">11.4%</span>
                    </div>
                    <div class="task-card">
                        <h4>HCC 108 (Vascular Disease)</h4>
                        <p>Binary classification for vascular disease</p>
                        <span class="prevalence">22.6%</span>
                    </div>
                    <div class="task-card">
                        <h4>HCC 12 (Cancers)</h4>
                        <p>Binary classification for breast, prostate, and other cancers</p>
                        <span class="prevalence">10.9%</span>
                    </div>
                    <div class="task-card">
                        <h4>HCC 96 (Cardiac Arrhythmias)</h4>
                        <p>Binary classification for cardiac arrhythmias</p>
                        <span class="prevalence">9.0%</span>
                    </div>
                    <div class="task-card">
                        <h4>HCC 18 (Diabetes w/ Complications)</h4>
                        <p>Binary classification for diabetes with chronic complications</p>
                        <span class="prevalence">8.3%</span>
                    </div>
                    <div class="task-card">
                        <h4>HCC 111 (COPD)</h4>
                        <p>Binary classification for chronic obstructive pulmonary disease</p>
                        <span class="prevalence">7.1%</span>
                    </div>
                    <div class="task-card">
                        <h4>Calcium Score</h4>
                        <p>Binary classification for Agatston score > 1000</p>
                        <span class="prevalence">21.2%</span>
                    </div>
                    <div class="task-card">
                        <h4>Myocardial Infarction</h4>
                        <p>Binary classification for previous MI</p>
                        <span class="prevalence">23.1%</span>
                    </div>
                    <div class="task-card">
                        <h4>Type 2 Diabetes</h4>
                        <p>Binary classification for diabetes at scan time</p>
                        <span class="prevalence">44.6%</span>
                    </div>
                    <div class="task-card">
                        <h4>Age Prediction</h4>
                        <p>Regression task for patient age at scan time</p>
                        <span class="prevalence">Mean: 55.3 years</span>
                    </div>
                </div>

                <h3>Architectures Evaluated</h3>
                <div class="arch-grid">
                    <div class="arch-card">
                        <h4>CNN Architectures</h4>
                        <ul>
                            <li>ResNet-18</li>
                            <li>ResNet-34</li>
                            <li>ResNet-50 (RadImageNet)</li>
                            <li>DenseNet-121</li>
                            <li>EfficientNet-B0</li>
                        </ul>
                    </div>
                    <div class="arch-card">
                        <h4>Vision Transformers</h4>
                        <ul>
                            <li>ViT-Small (DINOv2)</li>
                            <li>Swin Transformer-Base</li>
                        </ul>
                    </div>
                </div>

                <h3>Training Protocol</h3>
                <p>
                    We establish a standardized training protocol for fair and reproducible comparison:
                </p>
                <ul>
                    <li><strong>Optimizer:</strong> AdamW with weight decay 1×10⁻⁴</li>
                    <li><strong>Learning Rate:</strong> Evaluated across 1×10⁻⁵, 1×10⁻⁴, 1×10⁻³</li>
                    <li><strong>Batch Size:</strong> 16</li>
                    <li><strong>Training:</strong> 100 epochs with early stopping (patience: 10)</li>
                    <li><strong>Dropout:</strong> 0.2</li>
                    <li><strong>Class Imbalance:</strong> Inverse frequency weighting, balanced batch sampling, threshold optimization</li>
                </ul>
            </div>
        </section>

        <section id="results" class="section alt-bg">
            <div class="container">
                <h2>Results</h2>
                
                <h3>Key Findings</h3>
                <div class="results-highlight">
                    <div class="result-card highlight">
                        <h4>Age Prediction</h4>
                        <p class="metric">MAE: 6.223 years</p>
                        <p class="metric">R²: 0.757</p>
                        <p class="model">Best: EfficientNet-B0</p>
                    </div>
                    <div class="result-card highlight">
                        <h4>Mortality Prediction</h4>
                        <p class="metric">AUROC: 0.839</p>
                        <p class="model">Best: ResNet-18</p>
                    </div>
                    <div class="result-card highlight">
                        <h4>Diabetes w/ Complications</h4>
                        <p class="metric">AUROC: 0.801</p>
                        <p class="model">Best: Swin Transformer-Base</p>
                    </div>
                </div>

                <h3>Architectural Insights</h3>
                <ul>
                    <li><strong>Smaller architectures consistently matched or surpassed larger models:</strong> ResNet-18/34 and EfficientNet-B0 often outperformed ResNet-50 despite having fewer parameters</li>
                    <li><strong>Medical-domain pretraining (RadImageNet) did not outperform standard ImageNet pretraining</strong> on this task, suggesting that surface geometry representations differ from typical medical imaging</li>
                    <li><strong>Self-supervised pretraining (DINOv2) showed competitive performance</strong> but did not achieve the best results on any biomarker</li>
                    <li><strong>Vision Transformers (ViT-Small, Swin) demonstrated robustness</strong> across tasks, with Swin achieving best performance on several biomarkers</li>
                </ul>

                <h3>Performance Summary</h3>
                <div class="results-table">
                    <table>
                        <thead>
                            <tr>
                                <th>Task</th>
                                <th>Best Model</th>
                                <th>Metric</th>
                                <th>Performance</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Age</td>
                                <td>EfficientNet-B0</td>
                                <td>MAE</td>
                                <td>6.223 years</td>
                            </tr>
                            <tr>
                                <td>Mortality</td>
                                <td>ResNet-18</td>
                                <td>AUROC</td>
                                <td>0.839</td>
                            </tr>
                            <tr>
                                <td>Calcium Score</td>
                                <td>ResNet-34</td>
                                <td>AUROC</td>
                                <td>0.848</td>
                            </tr>
                            <tr>
                                <td>Myocardial Infarction</td>
                                <td>Swin Transformer-Base</td>
                                <td>AUROC</td>
                                <td>0.742</td>
                            </tr>
                            <tr>
                                <td>Type 2 Diabetes</td>
                                <td>ResNet-34</td>
                                <td>AUROC</td>
                                <td>0.742</td>
                            </tr>
                            <tr>
                                <td>HCC 108 (Vascular Disease)</td>
                                <td>Swin Transformer-Base</td>
                                <td>AUROC</td>
                                <td>0.768</td>
                            </tr>
                            <tr>
                                <td>HCC 111 (COPD)</td>
                                <td>ResNet-18</td>
                                <td>AUROC</td>
                                <td>0.769</td>
                            </tr>
                            <tr>
                                <td>HCC 18 (Diabetes w/ Chronic Complications)</td>
                                <td>Swin Transformer-Base</td>
                                <td>AUROC</td>
                                <td>0.801</td>
                            </tr>
                            <tr>
                                <td>HCC 96 (Cardiac Arrhythmias)</td>
                                <td>Swin Transformer-Base</td>
                                <td>AUROC</td>
                                <td>0.770</td>
                            </tr>
                            <tr>
                                <td>HCC 12 (Cancers)</td>
                                <td>ResNet-34</td>
                                <td>AUROC</td>
                                <td>0.591</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </section>

        <section id="download" class="section">
            <div class="container">
                <h2>Download & Access</h2>
                <div class="download-info">
                    <p>
                        We are working diligently to make the dataset publicly available as soon as possible. The release to include:
                    </p>
                    <ul>
                        <li>Full dataset (23,506 surface mesh images in PNG format)</li>
                        <li>3D surface meshes obtained from CT DICOM image series in STL format</li>
                        <li>HIPAA-compliant de-identified labels: 87 comorbidities, 31 diagnoses, 16 biomarkers</li>
                        <li>Data splits (train/validation/test)</li>
                        <li>Evaluation protocols and scripts</li>
                        <li>Pretrained model checkpoints for all architectures</li>
                        <li>Complete DICOM-to-STL-to-PNG processing pipeline</li>
                        <li>Biomarker extraction scripts that use OSCAR</li>
                    </ul>
                    <p>
                        <strong>GitHub Repository:</strong> Link to repository will be provided shortly
                    </p>
                    <p>
                        For access requests or questions, please contact us through GitHub issues or email <a href="mailto:mahmedch@stanford.edu">mahmedch@stanford.edu</a>.
                    </p>
                </div>
            </div>
        </section>

        <section id="citation" class="section alt-bg">
            <div class="container">
                <h2>Citation</h2>
                <div class="citation-box">
                    <pre><code>@inproceedings{abdctbench2026,
  title={AbdCTBench: Learning Clinical Biomarker Representations from Abdominal Surface Geometry},
  author={Chaudhry, Muhammad Ahmed and Bedi, Suhana and Lagari, Pola Lydia and Layden, Brian T and Galanter, William and Pyrros, Ayis and Koyejo, Sanmi},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2026}
}</code></pre>
                </div>
                <p class="citation-note">
                    <em>Accepted to ICLR 2026. For correspondence, contact Muhammad Ahmed Chaudhry at <a href="mailto:mahmedch@stanford.edu">mahmedch@stanford.edu</a>.</em>
                </p>
                <p class="citation-note">
                    <em>OpenReview: <a href="https://openreview.net/forum?id=dKRAo0a9Gm&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2026%2FConference%2FAuthors%23your-submissions)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=dKRAo0a9Gm</a></em>
                </p>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 Stanford University. AbdCTBench</p>
            <p class="stanford-footer">
                Stanford Trustworthy AI Research (STAIR) Lab
            </p>
            <p class="disclaimer">
                This dataset is provided for research purposes only. All data has been de-identified in accordance with HIPAA Safe Harbor requirements.
            </p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>

